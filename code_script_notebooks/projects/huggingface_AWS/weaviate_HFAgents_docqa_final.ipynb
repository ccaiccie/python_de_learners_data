{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lZtEmmTFdOHr"
      },
      "outputs": [],
      "source": [
        "!pip install weaviate-client sentence-transformers langchain huggingface_hub>=0.14.1 git+https://github.com/huggingface/transformers@v4.29.0 -q diffusers accelerate datasets torch soundfile sentencepiece opencv-python openai > /dev/null\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "vTUDDr8FgWQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import (\n",
        "    HuggingFaceEmbeddings, \n",
        "    SentenceTransformerEmbeddings\n",
        ")\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from langchain.vectorstores import Weaviate"
      ],
      "metadata": {
        "id": "OGWSWaj-gV8C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_splits(text_file):\n",
        "  \"\"\"Function takes in the text data and returns the  \n",
        "  splits so for further processing can be done.\"\"\"\n",
        "  with open(text_file,'r') as txt:\n",
        "    data = txt.read()\n",
        "\n",
        "  textSplit = RecursiveCharacterTextSplitter(chunk_size=150,\n",
        "                                             chunk_overlap=15,\n",
        "                                             length_function=len)\n",
        "  doc_list = textSplit.split_text(data)\n",
        "  return doc_list"
      ],
      "metadata": {
        "id": "jbVqUaWPgV3j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing out the above function with the open source \n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "fGBpCgwFgVzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "from weaviate.embedded import EmbeddedOptions\n",
        "\n",
        "client = weaviate.Client(\n",
        "  embedded_options=EmbeddedOptions(),\n",
        "  additional_headers={\n",
        "        \"X-HuggingFace-Api-Key\": \"\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd0d_YBSgVwg",
        "outputId": "b360a7a9-e59c-493f-e177-dc63b3fb0b1d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedded weaviate is already listening on port 6666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.schema.delete_all()\n",
        "\n",
        "client.schema.create_class(\n",
        "    {\n",
        "        \"class\": \"Sphere\",\n",
        "        \"description\" : \"Sphere vectorizer pipeline\",\n",
        "        \"moduleConfig\": {\n",
        "        \"text2vec-huggingface\": {\n",
        "          \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "          \"options\": {\n",
        "            \"waitForModel\": True,\n",
        "            \"useGPU\": False,\n",
        "            \"useCache\": True\n",
        "            }\n",
        "          }\n",
        "        },\n",
        "        \"properties\": [\n",
        "            {\n",
        "                \"name\": \"raw\",\n",
        "                \"dataType\": [\"text\"]\n",
        "            }\n",
        "        ],\n",
        "     \"vectorizer\":\"text2vec-huggingface\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "-c3wxW0zhynX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e133d999-4911-4b23-da69-7010e5581580"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded weaviate wasn't listening on port 6666, so starting embedded weaviate again\n",
            "Started /root/.cache/weaviate-embedded: process ID 5361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mail_docs = get_text_splits(\"/content/mail_collector.txt\")"
      ],
      "metadata": {
        "id": "K6EoC42Rhx8E"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mail_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lvg78fCRmBN0",
        "outputId": "509b56dc-cb90-4634-a8ad-3893c6109743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Space via IFTTT <action@ifttt.com>\\nAstronomy Picture of the Day:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Configure a batch process\n",
        "with client.batch as batch:\n",
        "    batch.batch_size=5\n",
        "    batch = 0\n",
        "    for iterator in mail_docs:\n",
        "        print(f\"processing batch {batch}\")\n",
        "        properties = {\n",
        "            \"raw\": iterator,\n",
        "        }\n",
        "        time.sleep(8)\n",
        "        client.batch.add_data_object(properties,\"Sphere\")"
      ],
      "metadata": {
        "id": "AJpi7SN5l52B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_docs(question:str):\n",
        "  nearText = {\"concepts\": [question]}\n",
        "  result = (\n",
        "      client.query\n",
        "      .get(\"Sphere\", [\"raw\"])\n",
        "      .with_near_text(nearText)\n",
        "      .with_limit(5)\n",
        "      .do()\n",
        "  )\n",
        "  temp_result = result['data']['Get']['Sphere']\n",
        "  print(temp_result)\n",
        "  text = ''\n",
        "  for txt in temp_result:\n",
        "    text = text + txt['raw']\n",
        "  return text"
      ],
      "metadata": {
        "id": "P5Uat0mioAOf"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "update_result = get_text_docs(\"photographs taken\")"
      ],
      "metadata": {
        "id": "4r_VUME5pQ9c"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_template = \"\"\"I will ask you to perform a task, your job is to come up with a series of simple commands in Python that will perform the task.\n",
        "To help you, I will give you access to a set of tools that you can use. Each tool is a Python function and has a description explaining the task it performs, \n",
        "the inputs it expects and the outputs it returns.\n",
        "You should first explain which tool you will use to perform the task and for what reason, then write the code in Python.\n",
        "Each instruction in Python should be a simple assignment. You can print intermediate results if it makes sense to do so.\n",
        "\n",
        "Tools:\n",
        "- text_qa: This is a tool that answers a question from a given `text`. \n",
        "It takes an input named `text` which should be the text containing the information, \n",
        "as well as a `question` that is the question about the text. \n",
        "It returns a text that contains the answer to the question.\n",
        "- document_qa: This is a tool that answers a question about a document (pdf). It takes an input named `document` which should be the document containing the information, as well as a `question` that is the question about the document. It returns a text that contains the answer to the question.\n",
        "- image_captioner: This is a tool that generates a description of an image. It takes an input named `image` which should be the image to the caption and returns a text that contains the description in English.\n",
        "\n",
        "\n",
        "Task: \"Answer the question in the variable `question` about the text in the variable `text`. Use the answer to generate an image.\"\n",
        "\n",
        "I will use the following tools: `text_qa` to create the answer, then `image_generator` to generate an image according to the answer.\n",
        "\n",
        "Answer:\n",
        "```py\n",
        "answer = text_qa(text=text, question=question)\n",
        "print(f\"The answer is {answer}.\")```\n",
        "\n",
        "Task: \"Identify the oldest person in the `document` and create an image showcasing the result as a banner.\"\n",
        "\n",
        "I will use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\n",
        "\n",
        "Answer:\n",
        "```py\n",
        "answer = document_qa(document, question=\"What is the oldest person?\")\n",
        "print(f\"The answer is {answer}.\")\n",
        "image = image_generator(\"A banner showing \" + answer)\n",
        "```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EVBOMvwIwc8p"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.tools import HfAgent\n",
        "plain_agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")"
      ],
      "metadata": {
        "id": "gmBILqmr1zRe"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plain_agent.run(task=\"\"\"Task is to answer following question.\"\"\", \n",
        "                    question= \"\"\"How many photographs are taken?\"\"\",\n",
        "                    text=\"\"\"2023-04-26 10:44:41+05:30Was this a lucky shot? Although many amazing photographs are taken by someone who just happenedÂ to be in the right place atthat it took many hours of exposure with a telescope in Seven Persons, Alberta , Canada to create the featured image.April 24, 2023via NASAright place at the right time, this image took skill and careful planning. First was the angularÂ scale: if you shoot too close to the famous Arc deplanning was successful, bringing two of humanity's most famous icons photographically together for all to enjoy.April 26, 2023via NASAof time \n",
        "                    -- from this distance less \n",
        "                    than a minute. Other planned features include lighting, relative brightness, \n",
        "                    height, capturing a good foreground\"\"\",\n",
        "                   return_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "wAaCZtKK169m",
        "outputId": "f99f0ac3-3a3b-44db-cc4c-74ce3497c4af"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==Explanation from the agent==\n",
            "I will use the following  tools: `text_qa` to create the answer.\n",
            "\n",
            "\n",
            "==Code generated by the agent==\n",
            "answer = text_qa(text=text, question=question)\n",
            "print(f\"The answer is {answer}.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from transformers import load_tool\\n\\ntext_qa = load_tool(\"text-question-answering\")\\n\\nanswer = text_qa(text=text, question=question)\\nprint(f\"The answer is {answer}.\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plain_agent.run(task=\"\"\"Task is to answer following question.\"\"\", \n",
        "                    question= \"\"\"How many photographs are taken?\"\"\",\n",
        "                    text=\"\"\"2023-04-26 10:44:41+05:30Was this a lucky shot? Although many amazing photographs are taken by someone who just happenedÂ to be in the right place atthat it took many hours of exposure with a telescope in Seven Persons, Alberta , Canada to create the featured image.April 24, 2023via NASAright place at the right time, this image took skill and careful planning. First was the angularÂ scale: if you shoot too close to the famous Arc deplanning was successful, bringing two of humanity's most famous icons photographically together for all to enjoy.April 26, 2023via NASAof time \n",
        "                    -- from this distance less \n",
        "                    than a minute. Other planned features include lighting, relative brightness, \n",
        "                    height, capturing a good foreground\"\"\")"
      ],
      "metadata": {
        "id": "AmacMouo2FLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_qa_fun(user_question):\n",
        "  text_docs = get_text_docs(user_question)\n",
        "  #print(text_docs)\n",
        "  answer = plain_agent.run(task=\"Task is to answer following question\", \n",
        "                question=user_question,\n",
        "                  text=text_docs)\n",
        "  print(answer)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "YicbF0hrhxz9"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_qa_fun(\"How many pictures are taken?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Upe6Ljl2hxt0",
        "outputId": "88bc7d53-c385-4bcd-e9f9-f19dc3823cce"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==Explanation from the agent==\n",
            "I will use the following  tools: `text_qa` to create the answer.\n",
            "\n",
            "\n",
            "==Code generated by the agent==\n",
            "answer = text_qa(text=text, question=question)\n",
            "print(f\"The answer is {answer}.\")\n",
            "\n",
            "\n",
            "==Result==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer is many amazing photographs are taken by someone who just happened to be in the right place atof.\n",
            "many amazing photographs are taken by someone who just happened to be in the right place atof\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'many amazing photographs are taken by someone who just happened to be in the right place atof'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_qa_fun(\"How the sky must be?\")"
      ],
      "metadata": {
        "id": "yzi5sJMEhxrD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}